{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8f5359-3775-4d81-a3b1-d18440aeae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3981e93a-9643-414b-a154-a665e0d3e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\n",
    "        \"system\",\n",
    "        \"You are an expert in data science and a helpful assistance\\\n",
    "        Your task is to provide clear, concise and easy to understand response to user.\"\n",
    "      ),\n",
    "      (\"user\",\"Question: {question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b6ee4-280d-4539-b17d-6dfdb81b7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanis\\AppData\\Local\\Temp\\ipykernel_4592\\2249803467.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "llm=Ollama(model=\"llama3.2\")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt | llm | output_parser\n",
    "def answer_question(input_text):\n",
    "    if input_text:\n",
    "      response=chain.invoke({\"question\":input_text})\n",
    "      return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4329b31e-be0d-4991-8550-59e944aeca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://279e9d5b32866467fc.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://279e9d5b32866467fc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\http\\client.py\", line 1298, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\http\\client.py\", line 1058, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\http\\client.py\", line 996, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002D4D282F150>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002D4D282F150>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 940, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\AppData\\Local\\Temp\\ipykernel_4592\\2249803467.py\", line 6, in answer_question\n",
      "    response=chain.invoke({\"question\":input_text})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 389, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 766, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 973, in generate\n",
      "    return self._generate_helper(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 792, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 437, in _generate\n",
      "    final_chunk = super()._stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 349, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 194, in _create_generate_stream\n",
      "    yield from self._create_stream(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 252, in _create_stream\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanis\\anaconda3\\envs\\llms\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002D4D282F150>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(label=\"Search the topic you want\"),\n",
    "    outputs=gr.Textbox(label=\"LLM Response\"),\n",
    "    title=\"LLM basics using OLLAMA\",\n",
    "    allow_flagging=\"never\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7392459-e0a7-4cc8-88fe-63f7c417df46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00b30c-12cd-4678-9754-b2f83b33d86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
